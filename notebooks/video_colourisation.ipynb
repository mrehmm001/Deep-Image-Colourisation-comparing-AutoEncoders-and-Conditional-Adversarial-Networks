{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "video_colourisation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "oVptqgOVTU__"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TKDjFrInTSv2"
      },
      "outputs": [],
      "source": [
        "#Data manipulation \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#Image processing / manipulation\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "import cv2\n",
        "\n",
        "#Deep learning libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras.layers import Conv2D, Activation,Reshape, UpSampling2D,Conv2DTranspose\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "#Utility libraries\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions"
      ],
      "metadata": {
        "id": "_4lichPaTZNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def normalize(input_image, real_image):\n",
        "  input_image = input_image/256\n",
        "  real_image = real_image/256\n",
        "\n",
        "  return input_image, real_image\n",
        "\n",
        "\n",
        "def deprocess(imgs):\n",
        "    imgs = imgs * 255\n",
        "    imgs = np.array(imgs)\n",
        "    imgs[imgs>255] = 255\n",
        "    imgs[imgs < 0] = 0\n",
        "    return imgs.astype(np.uint8)  \n",
        "\n",
        "def read_img(img):\n",
        "    res = []\n",
        "    for i,image in enumerate(img.numpy()):\n",
        "      labimg = cv2.cvtColor(cv2.resize(image, (224, 224)), cv2.COLOR_BGR2Lab)\n",
        "      labimg = labimg[:,:,0]\n",
        "      labimg = labimg.reshape(labimg.shape+(1,))\n",
        "      res.append(labimg)\n",
        "    res = np.array(res)\n",
        "    return res\n",
        "\n",
        "\n",
        "def reconstruct(batchX, predictedY):\n",
        "    result = np.concatenate((batchX, predictedY), axis=-1)\n",
        "    res = []\n",
        "    for i,image in enumerate(result):\n",
        "      rgbimg = cv2.cvtColor(image, cv2.COLOR_Lab2RGB)\n",
        "      res.append(rgbimg)\n",
        "    res = np.array(res)\n",
        "    return res\n",
        "\n",
        "\n",
        "#Essentials for global autoencoder\n",
        "\n",
        "class FusionLayer(Layer):\n",
        "    def call(self, inputs, mask=None):\n",
        "        imgs, embs = inputs\n",
        "        # reshaped_shape = imgs.shape[:3].concatenate(embs.shape[1])\n",
        "        reshaped_shape = (tf.shape(imgs)[0],imgs.shape[1],imgs.shape[1],embs.shape[1])\n",
        "        embs = K.repeat(embs, imgs.shape[1] * imgs.shape[2])\n",
        "        embs = K.reshape(embs, tf.stack(reshaped_shape))\n",
        "        return K.concatenate([imgs, embs], axis=3)\n",
        "\n",
        "    def compute_output_shape(self, input_shapes):\n",
        "        # Must have 2 tensors as input\n",
        "        assert input_shapes and len(input_shapes) == 2\n",
        "        imgs_shape, embs_shape = input_shapes\n",
        "\n",
        "        # The batch size of the two tensors must match\n",
        "        assert imgs_shape[0] == embs_shape[0]\n",
        "\n",
        "        # (batch_size, width, height, embedding_len + depth)\n",
        "        return imgs_shape[:3] + (imgs_shape[3] + embs_shape[1],)\n",
        "\n",
        "def getGlobal_encoder(model_input):\n",
        "  from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "  img_conc = layers.Concatenate()([model_input, model_input, model_input])  \n",
        "  inputModel = InceptionResNetV2(weights='imagenet',\n",
        "                      include_top=True,\n",
        "                      input_tensor=img_conc)\n",
        "  inputModel.trainable = False\n",
        "  return inputModel.output\n",
        "\n",
        "#For loading models\n",
        "def model():\n",
        "    from tensorflow.keras import Input\n",
        "    input = Input(shape=(256, 256, 1))\n",
        "    #The encoder\n",
        "    encoder = Conv2D(64, (3, 3), activation='relu', padding='same', strides=2)(input)\n",
        "    encoder = Conv2D(128, (3, 3), activation='relu', padding='same')(encoder)\n",
        "    encoder = Conv2D(128, (3, 3), activation='relu', padding='same', strides=2)(encoder)\n",
        "    encoder = Conv2D(256, (3, 3), activation='relu', padding='same')(encoder)\n",
        "    encoder = Conv2D(256, (3, 3), activation='relu', padding='same', strides=2)(encoder)\n",
        "    encoder = Conv2D(512, (3, 3), activation='relu', padding='same')(encoder)\n",
        "    encoder = Conv2D(512, (3, 3), activation='relu', padding='same')(encoder)\n",
        "    #Global encoder\n",
        "    globalEncoder = getGlobal_encoder(input)\n",
        "\n",
        "\n",
        "    #fusion\n",
        "    fusion = FusionLayer()([encoder,globalEncoder])\n",
        "    fusion = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(fusion)\n",
        "\n",
        "    #Decoder\n",
        "    decoder = Conv2D(128, (3, 3), activation='relu', padding='same')(fusion)\n",
        "    decoder = UpSampling2D((2, 2))(decoder)\n",
        "    decoder = Conv2D(64, (3, 3), activation='relu', padding='same')(decoder)\n",
        "    decoder = UpSampling2D((2, 2))(decoder)\n",
        "    decoder = Conv2D(32, (3, 3), activation='relu', padding='same')(decoder)\n",
        "    decoder = Conv2D(16, (3, 3), activation='relu', padding='same')(decoder)\n",
        "    decoder = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder)\n",
        "    decoder = UpSampling2D((2, 2))(decoder)\n",
        "    return Model(input,decoder)\n",
        "\n",
        "\n",
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result    \n",
        "  \n",
        "\n",
        "\n",
        "def Generator():\n",
        "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
        "\n",
        "  down_stack = [\n",
        "    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
        "    downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
        "    downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
        "    downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
        "    downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
        "    downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
        "    downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
        "    downsample(512, 4),  # (batch_size, 1, 1, 512)\n",
        "  ]\n",
        "\n",
        "  up_stack = [\n",
        "    upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
        "    upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
        "    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
        "    upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
        "    upsample(256, 4),  # (batch_size, 32, 32, 512)\n",
        "    upsample(128, 4),  # (batch_size, 64, 64, 256)\n",
        "    upsample(64, 4),  # (batch_size, 128, 128, 128)\n",
        "  ]\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  last = tf.keras.layers.Conv2DTranspose(3, 4,\n",
        "                                         strides=2,\n",
        "                                         padding='same',\n",
        "                                         kernel_initializer=initializer,\n",
        "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
        "\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "\n",
        "\n",
        "def colorization_model():\n",
        "\n",
        "        input_img = Input((224,224,3))\n",
        "\n",
        "\n",
        "        # VGG16 without top layers\n",
        "        VGG_model = applications.vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "        model_ = Model(VGG_model.input,VGG_model.layers[-6].output)\n",
        "        model = model_(input_img)\n",
        "\n",
        "\n",
        "        # Global Features\n",
        "\n",
        "        global_features = keras.layers.Conv2D(512, (3, 3), padding='same', strides=(2, 2), activation='relu')(model)\n",
        "        global_features = keras.layers.BatchNormalization()(global_features)\n",
        "        global_features = keras.layers.Conv2D(512, (3, 3), padding='same', strides=(1, 1), activation='relu')(global_features)\n",
        "        global_features = keras.layers.BatchNormalization()(global_features)\n",
        "\n",
        "        global_features = keras.layers.Conv2D(512, (3, 3), padding='same', strides=(2, 2), activation='relu')(global_features)\n",
        "        global_features = keras.layers.BatchNormalization()(global_features)\n",
        "        global_features = keras.layers.Conv2D(512, (3, 3), padding='same', strides=(1, 1), activation='relu')(global_features)\n",
        "        global_features = keras.layers.BatchNormalization()(global_features)\n",
        "\n",
        "        global_features2 = keras.layers.Flatten()(global_features)\n",
        "        global_features2 = keras.layers.Dense(1024)(global_features2)\n",
        "        global_features2 = keras.layers.Dense(512)(global_features2)\n",
        "        global_features2 = keras.layers.Dense(256)(global_features2)\n",
        "        global_features2 = keras.layers.RepeatVector(28*28)(global_features2)\n",
        "        global_features2 = keras.layers.Reshape((28,28, 256))(global_features2)\n",
        "\n",
        "        global_featuresClass = keras.layers.Flatten()(global_features)\n",
        "        global_featuresClass = keras.layers.Dense(4096)(global_featuresClass)\n",
        "        global_featuresClass = keras.layers.Dense(4096)(global_featuresClass)\n",
        "        global_featuresClass = keras.layers.Dense(1000, activation='softmax')(global_featuresClass)\n",
        "\n",
        "        # Midlevel Features\n",
        "\n",
        "        midlevel_features = keras.layers.Conv2D(512, (3, 3),  padding='same', strides=(1, 1), activation='relu')(model)\n",
        "        midlevel_features = keras.layers.BatchNormalization()(midlevel_features)\n",
        "        midlevel_features = keras.layers.Conv2D(256, (3, 3),  padding='same', strides=(1, 1), activation='relu')(midlevel_features)\n",
        "        midlevel_features = keras.layers.BatchNormalization()(midlevel_features)\n",
        "\n",
        "        # fusion of (VGG16 + Midlevel) + (VGG16 + Global)\n",
        "        modelFusion = keras.layers.concatenate([midlevel_features, global_features2])\n",
        "\n",
        "        # Fusion + Colorization\n",
        "        outputModel =  keras.layers.Conv2D(256, (1, 1), padding='same', strides=(1, 1), activation='relu')(modelFusion)\n",
        "        outputModel =  keras.layers.Conv2D(128, (3, 3), padding='same', strides=(1, 1), activation='relu')(outputModel)\n",
        "\n",
        "        outputModel =  keras.layers.UpSampling2D(size=(2,2))(outputModel)\n",
        "        outputModel =  keras.layers.Conv2D(64, (3, 3), padding='same', strides=(1, 1), activation='relu')(outputModel)\n",
        "        outputModel =  keras.layers.Conv2D(64, (3, 3), padding='same', strides=(1, 1), activation='relu')(outputModel)\n",
        "\n",
        "        outputModel =  keras.layers.UpSampling2D(size=(2,2))(outputModel)\n",
        "        outputModel =  keras.layers.Conv2D(32, (3, 3), padding='same', strides=(1, 1), activation='relu')(outputModel)\n",
        "        outputModel =  keras.layers.Conv2D(2, (3, 3), padding='same', strides=(1, 1), activation='sigmoid')(outputModel)\n",
        "        outputModel =  keras.layers.UpSampling2D(size=(2,2))(outputModel)\n",
        "        final_model = Model(input_img,[outputModel, global_featuresClass])\n",
        "\n",
        "        return final_model\n",
        "\n",
        "\n",
        "#For colourising images\n",
        "def colouriseMethod1or2(image,model):\n",
        "    input = np.array([image])\n",
        "    _,HEIGHT,WIDTH,_=input.shape\n",
        "    L_original = rgb2lab(input)[:,:,:,0]/100\n",
        "    L_original = L_original.reshape(L_original.shape+(1,))\n",
        "    input = tf.image.resize(input, [256, 256],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    #Lab mapping for model 1 & 2\n",
        "    L = rgb2lab(input)[:,:,:,0]/100\n",
        "    L = L.reshape(L.shape+(1,))\n",
        "\n",
        "    pred = model(L)\n",
        "    pred = tf.image.resize(pred, [HEIGHT, WIDTH],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "    result = lab2rgb(tf.concat([L_original*100,pred*128],axis=-1))[0]\n",
        "\n",
        "    return result\n",
        "\n",
        "def colouriseMethod3(image):\n",
        "    input = np.array([image])\n",
        "    _,HEIGHT,WIDTH,_=input.shape\n",
        "    L_original = rgb2lab(input)[:,:,:,0]/100\n",
        "    L_original = L_original.reshape(L_original.shape+(1,))\n",
        "    input = tf.image.resize(input, [256, 256],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    pred = model3(input)\n",
        "    pred = rgb2lab(pred)[:,:,:,1:]\n",
        "    pred = tf.image.resize(pred, [HEIGHT, WIDTH],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "    result = lab2rgb(tf.concat([L_original*100,pred],axis=-1))[0]\n",
        "    return result\n",
        "\n",
        "def colouriseMethod4(image):\n",
        "    input = np.array([image])\n",
        "    _,HEIGHT,WIDTH,_=input.shape\n",
        "    L_original = rgb2lab(input)[:,:,:,0]/100\n",
        "    L_original = L_original.reshape(L_original.shape+(1,))\n",
        "    input = tf.image.resize(input, [256, 256],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "    \n",
        "    L2 = read_img(input)/100\n",
        "    pred, _  = model4(np.tile(L2,[1,1,1,3]))\n",
        "    pred = tf.image.resize(pred, [HEIGHT, WIDTH],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "    result = reconstruct(deprocess(L_original), deprocess(pred))\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "LG9nkMViTad_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load models"
      ],
      "metadata": {
        "id": "stTM-bFgThUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = tf.keras.models.load_model('/trained_models/model.h5')#AutoEncoder\n",
        "model2 = model()\n",
        "model2.load_weights('/trained_models/model2.h5')\n",
        "model3 = Generator()\n",
        "model3.load_weights('/trained_models/generator.h5')\n",
        "model4 = colorization_model()\n",
        "model4.load_weights('/trained_models/chroma_generator.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Ey33FyTjFo",
        "outputId": "2b09ac4f-b9cb-4832-cd6d-3df581f9b60f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video colourisation logic\n",
        "\n",
        "The logic below uses CV2's VideoWriter to manipulate frames within the video. Here, the logic loops through each frame and colourises the frame using either of the chosen models. The colourised video will be saved in the \"colourised.mp4\". "
      ],
      "metadata": {
        "id": "RWY4AQ-JTbS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"footage.mp4\" #provide a path to a video you want to colourise\n",
        "MODEL = 3 #0 - Simple AutoEncoder | 1 - Global AutoEncoder | 2 - Pix2Pix | 3 - ChromaGAN\n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture(PATH)\n",
        "frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D')\n",
        "out = cv2.VideoWriter('colourised.mp4',fourcc , 20.0, (int(cap.get(3)),int(cap.get(4))))\n",
        "with tqdm(total=frames) as pbar:\n",
        "  while(cap.isOpened()):\n",
        "      ret, frame = cap.read()\n",
        "      if ret==True:\n",
        "          image = np.array(frame)\n",
        "          image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "          image = image.astype(\"float32\")/256\n",
        "          if MODEL == 0:\n",
        "            colourisedFrame = colouriseMethod1or2(image,model1)\n",
        "            colourisedFrame = cv2.cvtColor(colourisedFrame, cv2.COLOR_RGB2BGR)\n",
        "            colourisedFrame = np.uint8(256 * colourisedFrame)\n",
        "          elif MODEL == 1:\n",
        "            colourisedFrame = colouriseMethod1or2(image,model2)\n",
        "            colourisedFrame = cv2.cvtColor(colourisedFrame, cv2.COLOR_RGB2BGR)\n",
        "            colourisedFrame = np.uint8(256 * colourisedFrame)\n",
        "          elif MODEL == 2:\n",
        "            colourisedFrame = colouriseMethod3(image)\n",
        "            colourisedFrame = cv2.cvtColor(colourisedFrame, cv2.COLOR_RGB2BGR)\n",
        "            colourisedFrame = np.uint8(256 * colourisedFrame)\n",
        "          elif MODEL == 3:\n",
        "            colourisedFrame = colouriseMethod4(image)\n",
        "            colourisedFrame = cv2.cvtColor(colourisedFrame, cv2.COLOR_RGB2BGR)\n",
        "          \n",
        "          out.write(colourisedFrame)\n",
        "          if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "              break\n",
        "          pbar.update(1)\n",
        "      else:\n",
        "          break\n",
        "\n",
        "# Release everything if job is finished\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upA1PnjFTe5Q",
        "outputId": "cd897c86-3d20-47fa-e421-1796981fd057"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 349/349 [06:04<00:00,  1.04s/it]\n"
          ]
        }
      ]
    }
  ]
}