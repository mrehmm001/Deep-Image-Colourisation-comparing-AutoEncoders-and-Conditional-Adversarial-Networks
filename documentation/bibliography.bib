@article{DBLP:journals/corr/ChengYS16,
  author    = {Zezhou Cheng and
               Qingxiong Yang and
               Bin Sheng},
  title     = {Deep Colorization},
  journal   = {CoRR},
  volume    = {abs/1605.00075},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.00075},
  eprinttype = {arXiv},
  eprint    = {1605.00075},
  timestamp = {Fri, 26 Feb 2021 08:18:16 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/ChengYS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{deepkoal2017,
  author          = {Federico Baldassarre, Diego Gonzalez-Morin, Lucas Rodes-Guirao},
  title           = {Deep-Koalarization: Image Colorization using CNNs and Inception-ResNet-v2},
  journal         = {ArXiv:1712.03400},
  url             = {https://arxiv.org/abs/1712.03400},
  year            = 2017,
  month           = dec
}



@book{ware1994mechanism,
  title={MECHANISM OF IMAGE DETERIORATION IN EARLY PHOTOGRAPHS.},
  author={Ware, M.},
  url={https://books.google.co.uk/books?id=sRssvwEACAAJ},
  year={1994},
  publisher={SCIENCE MUSEUM}
}



@article{Levin2004,
   abstract = {Colonization is a computer-assisted process of adding color to a monochrome image or movie. The process typically involves segmenting images into regions and tracking these regions across image sequences. Neither of these tasks can be performed reliably in practice; consequently, colonization requires considerable user intervention and remains a tedious, time-consuming, and expensive task. In this paper we present a simple colorization method that requires neither precise image segmentation, nor accurate region tracking. Our method is based on a simple premise: neighboring pixels in space-time that have similar intensities should have similar colors. We formalize this premise using a quadratic cost function and obtain an optimization problem that can be solved efficiently using standard techniques. In our approach an artist only needs to annotate the image with a few color scribbles, and the indicated colors are automatically propagated in both space and time to produce a fully colorized image or sequence. We demonstrate that high quality colorizations of stills and movie clips may be obtained from a relatively modest amount of user input.},
   author = {Anat Levin and Dani Lischinski and Yair Weiss},
   doi = {10.1145/1015706.1015780},
   issn = {07300301},
   issue = {3},
   journal = {ACM Transactions on Graphics},
   keywords = {Colorization,Recoloring,Segmentation},
   pages = {689-694},
   title = {Colorization using optimization},
   volume = {23},
   url = {https://www.researchgate.net/publication/2896183_Colorization_using_Optimization},
   year = {2004},
}



@article{sun,
   abstract = {Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object cate-gorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.},
   author = {Jianxiong Xiao and James Hays and Krista A Ehinger Aude and Oliva Antonio Torralba},
   title = {SUN Database: Large-scale Scene Recognition from Abbey to Zoo},
   url = {http://groups.csail.mit.edu/vision/SUN/.},
}


@web_page{landscape,
   author = {Black Mamba},
   title = {Landscape color and grayscale images | Kaggle},
   url = {https://www.kaggle.com/theblackmamba31/landscape-image-colorization},
   year = {2021},
}


@misc{goodfellow2014generative,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@web_page{introToGanJasonBrownlee,
   author = {Jason Brownlee},
   title = {A Gentle Introduction to Generative Adversarial Networks (GANs)},
   url = {https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/},
}

@web_page{IntroductionToAutoencoders,
   author = {Jeremy Jordan},
   journal = {Introduction to autoencoders},
   title = {Introduction to autoencoders.},
   url = {https://www.jeremyjordan.me/autoencoders/},
   year = {2018},
}

@article{2020,
   title={The Open Images Dataset V4},
   volume={128},
   ISSN={1573-1405},
   url={http://dx.doi.org/10.1007/s11263-020-01316-z},
   DOI={10.1007/s11263-020-01316-z},
   number={7},
   journal={International Journal of Computer Vision},
   publisher={Springer Science and Business Media LLC},
   author={Kuznetsova, Alina and Rom, Hassan and Alldrin, Neil and Uijlings, Jasper and Krasin, Ivan and Pont-Tuset, Jordi and Kamali, Shahab and Popov, Stefan and Malloci, Matteo and Kolesnikov, Alexander and et al.},
   year={2020},
   month={Mar},
   pages={1956–1981}
}




@web_page{jantic,
   title = {jantic/DeOldify: A Deep Learning based project for colorizing and restoring old images (and video!)},
   url = {https://github.com/jantic/DeOldify#what-is-nogan},
}




@misc{sangkloy2016scribbler,
      title={Scribbler: Controlling Deep Image Synthesis with Sketch and Color}, 
      author={Patsorn Sangkloy and Jingwan Lu and Chen Fang and Fisher Yu and James Hays},
      year={2016},
      eprint={1612.00835},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@article{10.1145/2897824.2925974,
author = {Iizuka, Satoshi and Simo-Serra, Edgar and Ishikawa, Hiroshi},
title = {Let There Be Color! Joint End-to-End Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925974},
doi = {10.1145/2897824.2925974},
abstract = {We present a novel technique to automatically colorize grayscale images that combines both global priors and local image features. Based on Convolutional Neural Networks, our deep network features a fusion layer that allows us to elegantly merge local information dependent on small image patches with global priors computed using the entire image. The entire framework, including the global and local priors as well as the colorization model, is trained in an end-to-end fashion. Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN. We leverage an existing large-scale scene classification database to train our model, exploiting the class labels of the dataset to more efficiently and discriminatively learn the global priors. We validate our approach with a user study and compare against the state of the art, where we show significant improvements. Furthermore, we demonstrate our method extensively on many different types of images, including black-and-white photography from over a hundred years ago, and show realistic colorizations.},
journal = {ACM Trans. Graph.},
month = {jul},
articleno = {110},
numpages = {11},
keywords = {convolutional neural network, colorization}
}




@Article{IizukaSIGGRAPH2016,
  author  = {Satoshi Iizuka and Edgar Simo-Serra and Hiroshi Ishikawa},
  title   = {{Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification}},
  journal = {ACM Transactions on Graphics (Proc. of SIGGRAPH 2016)},
  year    = {2016},
  volume  = {35},
  number  = {4},
  pages   = {110:1--110:11},
}

@Comment{jabref-meta: databaseType:bibtex;}



@web_page{lab&rgb,
   title = {RGB and Lab - RawPedia},
   url = {https://rawpedia.rawtherapee.com/RGB_and_Lab},
}


@book{franoischollet2017learning,
  added-at = {2018-08-01T08:16:18.000+0200},
  author = {Chollet, François},
  biburl = {https://www.bibsonomy.org/bibtex/231f94815ebbd65d3a31e4a69e818573e/jaeschke},
  interhash = {cfbfd3f93853a469e5e6978f61a74a0a},
  intrahash = {31f94815ebbd65d3a31e4a69e818573e},
  isbn = {9781617294433},
  keywords = {ai deep deeplearning learning ml},
  month = nov,
  publisher = {Manning},
  timestamp = {2021-05-19T08:35:34.000+0200},
  title = {Deep Learning with Python },
  year = 2017
}


@web_page{anon,
   author = {Anonymous},
   title = {image - Formula to determine perceived brightness of RGB color - Stack Overflow},
   url = {https://stackoverflow.com/questions/596216/formula-to-determine-perceived-brightness-of-rgb-color},
}

@web_page{Akira,
   author = {Akira.AI},
   title = {Adam Optimization Algorithm | Complete Guide - Akira AI},
   url = {https://www.akira.ai/glossary/adam-optimization},
}


@article{Zhou1109,
   abstract = {The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.},
   author = {Bolei Zhou and Agata Lapedriza and Aditya Khosla and Aude Oliva and Antonio Torralba},
   doi = {10.1109/TPAMI.2017.2723009},
   keywords = {Index Terms-Scene classification,deep feature,deep learning,image dataset,visual recognition},
   title = {IEEE Transactions on Pattern Analysis and Machine Intelligence Places: A 10 million Image Database for Scene Recognition},
   url = {http://www.ieee.org/publications_standards/publications/rights/index.html},
   year = {1109},
}


@article{DBLP:journals/corr/IsolaZZE16,
  author    = {Phillip Isola and
               Jun{-}Yan Zhu and
               Tinghui Zhou and
               Alexei A. Efros},
  title     = {Image-to-Image Translation with Conditional Adversarial Networks},
  journal   = {CoRR},
  volume    = {abs/1611.07004},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.07004},
  eprinttype = {arXiv},
  eprint    = {1611.07004},
  timestamp = {Mon, 13 Aug 2018 16:49:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/IsolaZZE16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1907-09837,
  author    = {Patricia Vitoria and
               Lara Raad and
               Coloma Ballester},
  title     = {ChromaGAN: An Adversarial Approach for Picture Colorization},
  journal   = {CoRR},
  volume    = {abs/1907.09837},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.09837},
  eprinttype = {arXiv},
  eprint    = {1907.09837},
  timestamp = {Tue, 30 Jul 2019 12:52:26 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-09837.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@book{10.5555/3153997,
author = {Gron, Aurlien},
title = {Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems},
year = {2017},
isbn = {1491962291},
publisher = {O'Reilly Media, Inc.},
edition = {1st},
abstract = {Graphics in this book are printed in black and white. Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworksscikit-learn and Tensor Flowauthor Aurlien Gron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. Youll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what youve learned, all you need is programming experience to get started. Explore the machine learning landscape, particularly neural nets Use scikit-learn to track an example machine-learning project end-to-end Explore several training models, including support vector machines, decision trees, random forests, and ensemble methods Use the Tensor Flow library to build and train neural nets Dive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learning Learn techniques for training and scaling deep neural nets Apply practical code examples without acquiring excessive machine learning theory or algorithm details}
}



@article{enwiki:1065626908,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@misc{enwiki:1072564905,
  title={Open Source Computer Vision Library},
  author={Itseez},
  year={2015},
  howpublished = {\url{https://github.com/itseez/opencv}}
}





@misc{brownlee_2021, 
    title={Gentle introduction to the adam optimization algorithm for deep learning}, url={https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam\%20is\%20a\%20replacement\%20optimization,sparse\%20gradients\%20on\%20noisy\%20problems.}, journal={Machine Learning Mastery}, author={Brownlee, Jason}, year={2021}, month={Jan}
 } 

 @misc{kronovet_2017, title={Objective Functions in Machine Learning}, url={http://kronosapiens.github.io/blog/2017/03/28/objective-functions-in-machine-learning.html}, journal={Objective functions in machine learning}, author={Kronovet, Daniel}, year={2017}, month={Mar}} 

@Article{electronics10050593,
    AUTHOR = {Zhou, Jianlong and Gandomi, Amir H. and Chen, Fang and Holzinger, Andreas},
    TITLE = {Evaluating the Quality of Machine Learning Explanations: A Survey on Methods and Metrics},
    JOURNAL = {Electronics},
    VOLUME = {10},
    YEAR = {2021},
    NUMBER = {5},
    ARTICLE-NUMBER = {593},
    URL = {https://www.mdpi.com/2079-9292/10/5/593},
    ISSN = {2079-9292},
    ABSTRACT = {The most successful Machine Learning (ML) systems remain complex black boxes to end-users, and even experts are often unable to understand the rationale behind their decisions. The lack of transparency of such systems can have severe consequences or poor uses of limited valuable resources in medical diagnosis, financial decision-making, and in other high-stake domains. Therefore, the issue of ML explanation has experienced a surge in interest from the research community to application domains. While numerous explanation methods have been explored, there is a need for evaluations to quantify the quality of explanation methods to determine whether and to what extent the offered explainability achieves the defined objective, and compare available explanation methods and suggest the best explanation from the comparison for a specific task. This survey paper presents a comprehensive overview of methods proposed in the current literature for the evaluation of ML explanations. We identify properties of explainability from the review of definitions of explainability. The identified properties of explainability are used as objectives that evaluation metrics should achieve. The survey found that the quantitative metrics for both model-based and example-based explanations are primarily used to evaluate the parsimony/simplicity of interpretability, while the quantitative metrics for attribution-based explanations are primarily used to evaluate the soundness of fidelity of explainability. The survey also demonstrated that subjective measures, such as trust and confidence, have been embraced as the focal point for the human-centered evaluation of explainable systems. The paper concludes that the evaluation of ML explanations is a multidisciplinary research topic. It is also not possible to define an implementation of evaluation metrics, which can be applied to all explanation methods.},
    DOI = {10.3390/electronics10050593}
}


 @misc{kumar_2022, title={Hold-out method for training machine learning models}, url={https://vitalflux.com/hold-out-method-for-training-machine-learning-model/}, journal={Data Analytics}, author={Kumar , Ajitesh}, year={2022}, month={Apr}} 

@article{Chauhan2018,
   author = {Kalpana Chauhan and Rajeev K. Chauhan and Anju Saini},
   doi = {10.1016/B978-0-12-813087-2.00004-X},
   isbn = {9780128130872},
   journal = {Soft Computing Based Medical Image Analysis},
   month = {1},
   pages = {61-79},
   publisher = {Elsevier Inc.},
   title = {Enhancement and despeckling of echocardiographic images},
   year = {2018},
}


 @misc{anon_2014, title={How to calculate PSNR (peak signal to Noise Ratio) in MATLAB?}, url={https://www.matlabclass.com/2014/02/how-to-calculate-psnr-peak-signal-to.html}, journal={MATLAB Projects of Digital Image processing, Audio Processing, Video Processing and Basics of MATLAB}, author={anon, anon}, year={2014}, month={Feb}} 
 
 

@InProceedings{enwiki:1078654743,
author="Yoo, Andy B.
and Jette, Morris A.
and Grondona, Mark",
editor="Feitelson, Dror
and Rudolph, Larry
and Schwiegelshohn, Uwe",
title="SLURM: Simple Linux Utility for Resource Management",
booktitle="Job Scheduling Strategies for Parallel Processing",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="44--60",
abstract="A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.",
isbn="978-3-540-39727-4"
}

 @web_page{autoencoder2019,
   author = {Mahmoud El-Jiddawi},
   title = {Auto Colorization of Black and White Images using Machine Learning “Auto-encoders” technique | by Mahmoud El-Jiddawi | Becoming Human: Artificial Intelligence Magazine},
   url = {https://becominghuman.ai/auto-colorization-of-black-and-white-images-using-machine-learning-auto-encoders-technique-a213b47f7339},
   year = {2019},
}


@web_page{enwiki:1085146109,
   author = {Sumit Saha},
   title = {A Comprehensive Guide to Convolutional Neural Networks — the ELI5 way | by Sumit Saha | Towards Data Science},
   url = {https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53},
   year = {2018},
}


  
@misc{berus_2018, url={https://www.researchgate.net/figure/Architecture-of-LeNet-5-a-Convolutional-Neural-Network-here-for-digits-recognition_fig1_2985446}, journal={DEEP LEARNING IN INDUSTRY}, publisher={researchgate}, author={Berus, Lucijano}, year={2018}, month={Dec}} 

   
@misc{flores_2019, title={Variational autoencoders are beautiful}, url={https://www.compthree.com/blog/autoencoder/}, journal={Variational Autoencoders are Beautiful | Blogs}, author={Flores, Steven}, year={2019}, month={Apr}} 

    

 @misc{mohanty_2019, title={Multi Layer Perceptron (MLP) models on Real World Banking Data}, url={https://becominghuman.ai/multi-layer-perceptron-mlp-models-on-real-world-banking-data-f6dd3d7e998f}, journal={Medium}, publisher={Becoming Human: Artificial Intelligence Magazine}, author={Mohanty, Awhan}, year={2019}, month={May}} 
 
 
 @InProceedings{RFB15a,
  author       = "O. Ronneberger and P.Fischer and T. Brox",
  title        = "U-Net: Convolutional Networks for Biomedical Image Segmentation",
  booktitle    = "Medical Image Computing and Computer-Assisted Intervention (MICCAI)",
  series       = "LNCS",
  volume       = "9351",
  pages        = "234--241",
  year         = "2015",
  publisher    = "Springer",
  note         = "(available on arXiv:1505.04597 [cs.CV])",
  url          = "http://lmb.informatik.uni-freiburg.de/Publications/2015/RFB15a"
}


 @misc{ao_2018, title={Architecture of the Patchgan discriminator network ...}, url={https://www.researchgate.net/figure/Architecture-of-the-PatchGAN-Discriminator-network_fig5_328150573}, journal={Dialectical GAN for SAR image translation}, author={Ao, Dongyang}, year={2018}, month={Oct}} 
 
@misc{etal_2018, title={Papers with code - patchgan explained}, url={https://paperswithcode.com/method/patchgan#:~:text=PatchGAN\%20is\%20a\%20type\%20of,image\%20is\%20real\%20or\%20fake.}, journal={Explained | Papers With Code}, author={et al, Isola}, year={2018}, month={Nov}} 


@misc{brownlee_2020, title={A gentle introduction to the rectified linear unit (ReLU)}, url={https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/}, journal={Machine Learning Mastery}, author={Brownlee, Jason}, year={2020}, month={Aug}} 

@web_page{enwiki:1073306062,
   title = {Understanding Generalization Error in Machine Learning | by Yi-xin | Medium},
   author="Yi-xin",
   url = {https://medium.com/@yixinsun_56102/understanding-generalization-error-in-machine-learning-e6c03b203036},
}




@misc{bagheri_2019, title={A tutorial on conditional generative adversarial nets + keras implementation}, url={https://medium.com/@ma.bagheri/a-tutorial-on-conditional-generative-adversarial-nets-keras-implementation-694dcafa6282}, journal={Medium}, publisher={Medium}, author={Bagheri, Mohammad Ali}, year={2019}, month={Jun}} 


 @misc{frumkin_manipula_kachai_fehr_sewell, title={Conditional generative adversarial network (cgan) - wiki}, url={https://golden.com/wiki/Conditional_generative_adversarial_network_(cGAN)-99B85NK#:~:text=Conditional\%20generative\%20adversarial\%20network\%20(cGAN)\%20is\%20an\%20extension\%20of\%20the,framework\%20for\%20training\%20generative\%20models.}, journal={Golden}, author={Frumkin, Daniel and Manipula, Melanie and Kachai, Krisorn and Fehr, Axel and Sewell, Dawson}} 
 
 
 
 @book{haykin2009neural,
  abstract = {Neural Networks and Learning Machines, Third Edition is renowned for its thoroughness and readability. This well-organized and completely up-to-date text remains the most comprehensive treatment of neural networks from an engineering perspective. This is ideal for professional engineers and research scientists.
 
Matlab codes used for the computer experiments in the text are available for download at: http://www.pearsonhighered.com/haykin/
 
Refocused, revised and renamed to reflect the duality of neural networks and learning machines, this edition recognizes that the subject matter is richer when these topics are studied together. Ideas drawn from neural networks and machine learning are hybridized to perform improved learning tasks beyond the capability of either independently.},
  added-at = {2017-03-18T17:31:57.000+0100},
  address = {Upper Saddle River, NJ},
  author = {Haykin, Simon S.},
  biburl = {https://www.bibsonomy.org/bibtex/2e5015812328aaeccd73d8b03a7e36831/vngudivada},
  edition = {Third},
  interhash = {4cef19efafc52ae42607f9832a205214},
  intrahash = {e5015812328aaeccd73d8b03a7e36831},
  keywords = {Book Learning NeuralNetwork},
  publisher = {Pearson Education},
  timestamp = {2019-03-25T17:09:32.000+0100},
  title = {Neural networks and learning machines},
  year = 2009
}


@article{DBLP:journals/corr/ZhangIE16,
  author    = {Richard Zhang and
               Phillip Isola and
               Alexei A. Efros},
  title     = {Colorful Image Colorization},
  journal   = {CoRR},
  volume    = {abs/1603.08511},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.08511},
  eprinttype = {arXiv},
  eprint    = {1603.08511},
  timestamp = {Wed, 14 Aug 2019 08:23:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZhangIE16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

 @misc{bhattiprolu_2020, title={Python_for_microscopists/090a-autoencoder_colorize_v0.2.py at master · bnsreenu/python_for_microscopists}, url={https://github.com/bnsreenu/python_for_microscopists/blob/master/090a-autoencoder_colorize_V0.2.py}, journal={GitHub 090a-autoencoder_colorize}, author={Bhattiprolu, Sreenivas}, year={2020}, month={Jan}} 
 
 
 @article{DBLP:journals/corr/abs-1807-06587,
  author    = {Mingming He and
               Dongdong Chen and
               Jing Liao and
               Pedro V. Sander and
               Lu Yuan},
  title     = {Deep Exemplar-based Colorization},
  journal   = {CoRR},
  volume    = {abs/1807.06587},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.06587},
  eprinttype = {arXiv},
  eprint    = {1807.06587},
  timestamp = {Thu, 09 Jan 2020 08:16:48 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1807-06587.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


 @misc{wallner_2021, title={How to colorize black &amp; white photos with just 100 lines of Neural Network Code}, url={https://emilwallner.medium.com/colorize-b-w-photos-with-a-100-line-neural-network-53d9b4449f8d}, journal={Medium}, publisher={Medium}, author={Wallner, Emil}, year={2021}, month={Feb}} 

@web_page{enwiki:1080368112,
   title = {What is the RGB color space?},
   author="Karstens, Frank",
   url = {https://www.baslerweb.com/en/sales-support/knowledge-base/frequently-asked-questions/what-is-the-rgb-color-space/15179/},
}




@Inbook{enwiki:1077444859,
author="Luo, Ming Ronnier",
editor="Luo, Ronnier",
title="CIELAB",
bookTitle="Encyclopedia of Color Science and Technology",
year="2014",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--7",
isbn="978-3-642-27851-8",
doi="10.1007/978-3-642-27851-8_11-1",
url="https://doi.org/10.1007/978-3-642-27851-8_11-1"
}





@INPROCEEDINGS{5596999,
  author={Horé, Alain and Ziou, Djemel},
  booktitle={2010 20th International Conference on Pattern Recognition}, 
  title={Image Quality Metrics: PSNR vs. SSIM}, 
  year={2010},
  volume={},
  number={},
  pages={2366-2369},
  doi={10.1109/ICPR.2010.579}}
  
  
  

@inproceedings{inproceedings,
author = {Blanch, Marc and Mrak, Marta and Smeaton, Alan and O’Connor, Noel},
year = {2019},
month = {09},
pages = {1-6},
title = {End-to-End Conditional GAN-based Architectures for Image Colourisation},
doi = {10.1109/MMSP.2019.8901712}
}


@misc{https://doi.org/10.48550/arxiv.2204.05200,
  doi = {10.48550/ARXIV.2204.05200},
  
  url = {https://arxiv.org/abs/2204.05200},
  
  author = {Mullery, Seán and Whelan, Paul F.},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Human vs Objective Evaluation of Colourisation Performance},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{anwar2020ColorSurvey,
  title={Image Colorization: A Survey and Dataset},
  author={Anwar, Saeed and Tahir, Muhammad and Li, Chongyi and Mian, Ajmal and Khan, Fahad Shahbaz and Muzaffar, Abdul Wahab},
  journal={arXiv preprint arXiv:2008.10774},
  year={2020}
}

@article{Li2019,
   abstract = {This chapter reviews the recent development of image colourisation, which aims at adding colour to a given greyscale image. There are numerous applications involving image colourisation, such as converting black and white photos or movies to colour, restoring historic photographs to improve the aesthetics of the image, as well as colourising many other types of images lacking colour (e.g. medical images, infrared night time images). According to the source where the colours come from, the existing methods can be categorised into three classes: colourisation by reference, colourisation by scribbles and colourisation by deep learning. In this chapter, we introduce the basic idea and survey the typical algorithms of each type of method.},
   author = {Bo Li and Yu-Kun Lai and Paul L Rosin},
   title = {Chapter 1 A review of image colourisation},
   url = {https://users.cs.cf.ac.uk/Paul.Rosin/resources/papers/colourisation-chapter.pdf},
   year = {2019},
}

@article{deepimagecolorizationwithuserguidence,
   abstract = {Colorization has been studied by researchers in the field of computer graphics and image processing since Wilson Markle colorized
pictures from the Apollo space program in 1970. Adding color to
photographs by hand is a tedious process, which requires that the
artist segment the image and then assign colors to each segment.
The aim of our project is to design an algorithm and interactive system that automatically colorizes a monochrome image with human
guidance. The algorithm takes a grayscale image and some color
scribbles drawn by a human and produces a fully colorized image
that is both consistent with the scribble and the image semantics.
While colorization itself has numerous applications such as image
enhancement and film restoration, the unsupervised learning techniques that allowed us to add color to images can also enable automatically learning information such as depth from an RGB image for applications such as autonomous driving. Colorization is a
useful research task for improving our understanding of the visual
world.
We are using images from the ImageNet data set. The ImageNet
data can be download from the ImageNet official website. It is a diverse dataset that includes images from dogs and trucks to animated
characters. To evaluate our algorithm, we will convert color images
to grayscale, add scribbles to the images, and compute the peak
signal-to-noise ratio between the predicted image and the ground
truth image. This is a common metric used to evaluate the performance of colorization algorithms.
},
   author = {Xinyang Geng, Angela Lin, Kevin Yu
},
   title = {Deep Image Colorization with User Guidance},
   year = {2019},
}




@web_page{nogan2020,
   author = {Jason Antic and Jeremy Howard and Uri Manor},
   title = {Decrappification, DeOldification, and Super Resolution · fast.ai},
   url = {https://www.fast.ai/2019/05/03/decrappify/},
   year = {2019},
}



@article{DBLP:journals/corr/ZhouKLTO16,
  author    = {Bolei Zhou and
               Aditya Khosla and
               {\`{A}}gata Lapedriza and
               Antonio Torralba and
               Aude Oliva},
  title     = {Places: An Image Database for Deep Scene Understanding},
  journal   = {CoRR},
  volume    = {abs/1610.02055},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.02055},
  eprinttype = {arXiv},
  eprint    = {1610.02055},
  timestamp = {Mon, 13 Aug 2018 16:47:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ZhouKLTO16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{1284395,
  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  journal={IEEE Transactions on Image Processing}, 
  title={Image quality assessment: from error visibility to structural similarity}, 
  year={2004},
  volume={13},
  number={4},
  pages={600-612},
  doi={10.1109/TIP.2003.819861}}
  
  
  @article{deepkoal2017code,
  author          = {Federico Baldassarre, Diego Gonzalez-Morin, Lucas Rodes-Guirao},
  title           = {Deep-Koalarization: Image Colorization using CNNs and Inception-ResNet-v2 GitHub},
  journal         = {ArXiv:1712.03400},
  url             = {https://github.com/baldassarreFe/deep-koalarization},
  year            = 2017,
  month           = dec
}

@inproceedings{vitoria2020chromagancode,
  title={ChromaGAN: Adversarial Picture Colorization with Semantic Class Distribution GitHub},
  author={Vitoria, Patricia and Raad, Lara and Ballester, Coloma},
  booktitle={The IEEE Winter Conference on Applications of Computer Vision},
  url = {https://github.com/pvitoria/ChromaGAN},
  pages={2445--2454},
  year={2020}
}

@article{pix2pix2017code,
  title={Image-to-Image Translation with Conditional Adversarial Networks GitHub},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  url = {https://github.com/phillipi/pix2pix},
  journal={CVPR},
  year={2017}
}

@article{mrehm001,
  title={Muneeb Rehman - Image colourisation using Convolution Neural Network (Interim Report) },
  author={Muneeb Rehman},
  year={2022}
}